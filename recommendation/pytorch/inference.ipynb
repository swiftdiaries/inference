{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import heapq\n",
    "import math\n",
    "import time\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import multiprocessing as mp\n",
    "\n",
    "import utils\n",
    "from neumf import NeuMF\n",
    "from dataset import CFTrainDataset, load_test_ratings, load_test_negs\n",
    "from convert import (TEST_NEG_FILENAME, TEST_RATINGS_FILENAME,\n",
    "                     TRAIN_RATINGS_FILENAME)\n",
    "\n",
    "from mlperf_compliance import mlperf_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, users, items, batch_size=1024, use_cuda=True):\n",
    "    batches = [(users[i:i + batch_size], items[i:i + batch_size])\n",
    "               for i in range(0, len(users), batch_size)]\n",
    "    preds = []\n",
    "    for user, item in batches:\n",
    "        def proc(x):\n",
    "            x = np.array(x)\n",
    "            x = torch.from_numpy(x)\n",
    "            if use_cuda:\n",
    "                x = x.cuda(async=True)\n",
    "            return torch.autograd.Variable(x)\n",
    "        outp = model(proc(user), proc(item), sigmoid=True)\n",
    "        outp = outp.data.cpu().numpy()\n",
    "        preds += list(outp.flatten())\n",
    "    return preds\n",
    "\n",
    "\n",
    "def _calculate_hit(ranked, test_item):\n",
    "    return int(test_item in ranked)\n",
    "\n",
    "\n",
    "def _calculate_ndcg(ranked, test_item):\n",
    "    for i, item in enumerate(ranked):\n",
    "        if item == test_item:\n",
    "            return math.log(2) / math.log(i + 2)\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def eval_one(rating, items, model, K, use_cuda=True):\n",
    "    user = rating[0]\n",
    "    test_item = rating[1]\n",
    "    items.append(test_item)\n",
    "    users = [user] * len(items)\n",
    "    predictions = predict(model, users, items, use_cuda=use_cuda)\n",
    "\n",
    "    map_item_score = {item: pred for item, pred in zip(items, predictions)}\n",
    "    ranked = heapq.nlargest(K, map_item_score, key=map_item_score.get)\n",
    "\n",
    "    hit = _calculate_hit(ranked, test_item)\n",
    "    ndcg = _calculate_ndcg(ranked, test_item)\n",
    "    return hit, ndcg, len(predictions)\n",
    "\n",
    "\n",
    "def val_epoch(model, ratings, negs, K, use_cuda=False, output=None, epoch=None,\n",
    "              processes=1):\n",
    "    if epoch is None:\n",
    "        print(\"Initial evaluation\")\n",
    "    else:\n",
    "        print(\"Epoch {} evaluation\".format(epoch))\n",
    "\n",
    "    mlperf_log.ncf_print(key=mlperf_log.EVAL_START, value=epoch)\n",
    "    start = datetime.now()\n",
    "    model.eval()\n",
    "    if processes > 1:\n",
    "        context = mp.get_context('spawn')\n",
    "        _eval_one = partial(eval_one, model=model, K=K, use_cuda=use_cuda)\n",
    "        with context.Pool(processes=processes) as workers:\n",
    "            hits_ndcg_numpred = workers.starmap(_eval_one, zip(ratings, negs))\n",
    "        hits, ndcgs, num_preds = zip(*hits_ndcg_numpred)\n",
    "    else:\n",
    "        hits, ndcgs, num_preds = [], [], []\n",
    "        for rating, items in zip(ratings, negs):\n",
    "            hit, ndcg, num_pred = eval_one(rating, items, model, K, use_cuda=use_cuda)\n",
    "            hits.append(hit)\n",
    "            ndcgs.append(ndcg)\n",
    "            num_preds.append(num_pred)\n",
    "\n",
    "    hits = np.array(hits, dtype=np.float32)\n",
    "    ndcgs = np.array(ndcgs, dtype=np.float32)\n",
    "\n",
    "    assert len(set(num_preds)) == 1\n",
    "    num_neg = num_preds[0] - 1  # one true positive, many negatives\n",
    "    mlperf_log.ncf_print(key=mlperf_log.EVAL_SIZE, value={\"epoch\": epoch, \"value\": len(hits) * (1 + num_neg)})\n",
    "    mlperf_log.ncf_print(key=mlperf_log.EVAL_HP_NUM_USERS, value=len(hits))\n",
    "    mlperf_log.ncf_print(key=mlperf_log.EVAL_HP_NUM_NEG, value=num_neg)\n",
    "\n",
    "    end = datetime.now()\n",
    "    if output is not None:\n",
    "        result = OrderedDict()\n",
    "        result['timestamp'] = datetime.now()\n",
    "        result['duration'] = end - start\n",
    "        result['epoch'] = epoch\n",
    "        result['K'] = K\n",
    "        result['hit_rate'] = np.mean(hits)\n",
    "        result['NDCG'] = np.mean(ndcgs)\n",
    "        utils.save_result(result, output)\n",
    "\n",
    "    return hits, ndcgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ":::MLPv0.5.0 ncf 1543949476.834055901 (neumf.py:19) model_hp_mf_dim\n",
      "\n",
      ":::MLPv0.5.0 ncf 1543949477.148610115 (neumf.py:27) model_hp_mlp_layer_sizes: [256, 256, 128, 64]\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "model = NeuMF(138493, 26744, mf_dim=64, mf_reg=0., mlp_layer_sizes=[256, 256, 128, 64], mlp_layer_regs=[0. for i in [256, 256, 128, 64]])\n",
    "model.load_state_dict(torch.load(\"./trained_models/ncf-model-1543320146.pt\", map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuMF(\n",
       "  (mf_user_embed): Embedding(138493, 64)\n",
       "  (mf_item_embed): Embedding(26744, 64)\n",
       "  (mlp_user_embed): Embedding(138493, 128)\n",
       "  (mlp_item_embed): Embedding(26744, 128)\n",
       "  (mlp): ModuleList(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  )\n",
       "  (final): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial evaluation\n",
      "\n",
      ":::MLPv0.5.0 ncf 1543949577.630151987 (<ipython-input-29-82dc7a95c78c>:51) eval_start\n"
     ]
    }
   ],
   "source": [
    "data = \"ml-20m\"\n",
    "topk = 10\n",
    "processes = 10\n",
    "test_ratings = load_test_ratings(os.path.join(data, TEST_RATINGS_FILENAME))\n",
    "test_negs = load_test_negs(os.path.join(data, TEST_NEG_FILENAME))\n",
    "hits, ndcgs = val_epoch(model, test_ratings, test_negs, topk,\n",
    "                            use_cuda=use_cuda, processes=1)\n",
    "print(\"Hits and ncdgs\", hits, ndcgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
